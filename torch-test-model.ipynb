{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7b4f18-60aa-4c27-a25c-79c69cf7b89f",
   "metadata": {},
   "source": [
    "## Seeing how our simple torch model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b377805e-2ec1-4905-9f73-12bbc9750e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports!\n",
    "# Mostly torch and torchvision utilities, with plotting and tqdm helpers.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aacd0b93-5c6e-4864-8f6e-85bc10d5a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# We'll assign the first GPU Device to our device variable.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # let's see what device we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddda446-80c9-4103-b5f8-b424a3347742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting set to import our trained model.\n",
    "\n",
    "# batch size of 1 so we can look at one image at time.\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 784)\n",
    "        self.fc2 = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c24aa3-0b70-4290-8e70-c1ef30775f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting the model on our GPU and loading in those trained weights.\n",
    "model = SimpleNet().to( device )\n",
    "model.load_state_dict( torch.load(\"mnist_fashion_SimpleNet.pt\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de877e72-f731-42c1-9fb6-2073af0c1fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
       "  (fc2): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a8c1ad-54f8-4e3b-a2c3-33ac10b57396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our loader as before - will have that same batch size of 1.\n",
    "\n",
    "test_transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "\n",
    "\n",
    "testset = datasets.FashionMNIST('./data', train=False,\n",
    "                   transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "testing_iterator = iter(test_loader) # create an iterator of our loader\n",
    "\n",
    "# A dictionary to map our class numbers to their items.\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f3e8ea3-b322-485c-a13d-c31e36d542e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(model, x, y, device):\n",
    "    \"\"\" Function to return the current probabilities of the classes given the model \"\"\"\n",
    "    model = model.to(device)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(x)  # model classifies the input\n",
    "        output = output.squeeze()\n",
    "        \n",
    "    # Return the list of probabilites and the probability of ground truth class\n",
    "    return output, output[y] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b842bd7-66a1-4feb-a0ee-45b3c4d09a82",
   "metadata": {},
   "source": [
    "#### Okay, we should have everything we need now. Let's examine some data points and see how the model performs!\n",
    "\n",
    "Run the below cell repeatedly to see if the model predicts the selected image correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d4b137b-5901-4d56-81b3-762257a23857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape is torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAP1klEQVR4nO3db4hddX7H8c/XmBjzD5NIxyEbGiuiLEVNDUFoqMqyS6oPzIos64OSUiGLrLALfVDZPlihFLR0t0+EQJbIppq6LKgoUty1YWnaJ6sxRo3abFKN7oSYGBI1CYkxybcP5mQ76pzfd7y/e++5+n2/YJiZ851zz29O5pN77v2ec37m7gLw1XdR1wMAMByEHUiCsANJEHYgCcIOJHHxMDdmZrz1DwyYu9t0y6ue2c1srZntMbN9ZnZ/zWMBGCzrtc9uZrMk/U7SNyVNSHpR0t3u/kZhHZ7ZgQEbxDP7akn73P0tdz8j6ReS7qh4PAADVBP2ZZJ+P+X7iWbZp5jZBjPbYWY7KrYFoNLA36Bz902SNkkcxgNdqnlmPyBp+ZTvv9YsAzCCasL+oqSrzexKM5sj6buSnunPsAD0W8+H8e5+1szuk/QrSbMkPeLur/dtZAD6qufWW08b4zU7MHADOakGwJcHYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0n0PGUz/t+9995brG/cuHFII8GoMJt2ItU/GObsyRdUhd3M9ks6LumcpLPuvqofgwLQf/14Zr/V3Y/04XEADBCv2YEkasPukn5tZi+Z2YbpfsDMNpjZDjPbUbktABVqD+PXuPsBM/sjSc+b2f+4+/apP+DumyRtkiQzG/67EgAkVT6zu/uB5vNhSU9JWt2PQQHov57DbmbzzWzhha8lfUvS7n4NDEB/1RzGj0l6quknXizp39z9ub6MagAuvrj8q549e7ZYv/XWW1trS5Ys6XldSTp37lyxvnfv3mL96NGjrbWPP/64uC4Go4s+eqTnsLv7W5Ku7+NYAAwQrTcgCcIOJEHYgSQIO5AEYQeSsGG2CAZ5Bt1FF5X/3zp//nzV+ps3b26tRftw3rx5xfqJEyeK9bGxsWJ9/vz5PT92JLpUc5Ci/Rq1S0v1RYsWFdeNfu+JiYli/eTJk8X6rFmzWms7d+4srlv6W5Qkd5928DyzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASX5k+e61HH320WF+6dGlr7YMPPiiuG/VsSz1XKb48d/bs2a21OXPmFNet7aNH5yfUPH50bkSk9LtH4z5ypHwP1ejciejfrHTp8ZkzZ4rrrlu3rlinzw4kR9iBJAg7kARhB5Ig7EAShB1IgrADSYzUlM01PdnofIEVK1YU69Htmi+77LLWWnRd9aWXXlqsz507t1iP+s2lW1GfOnWquG60z0s9/FrRtmv77KVeerTt2nsQLFiwoFg/ffp0ay06B2DNmjWttV27drU/bvFRAXxlEHYgCcIOJEHYgSQIO5AEYQeSIOxAEl+qPnup77pw4cLiulGf/dChQ8V6qc8eTYt8ySWXFOvROQLRtdE1/ejosaNr7aPppmvGFq0b7bfSdeHRY0f3AYj2W6R0/kJ0bkPpb73Uow+f2c3sETM7bGa7pyxbYmbPm9ne5vPi6HEAdGsmh/E/l7T2M8vul7TN3a+WtK35HsAIC8Pu7tslHf3M4jskbWm+3iJpXX+HBaDfen3hMebuB5uv35PUOhmZmW2QtKHH7QDok+o36NzdSzeSdPdNkjZJo33DSeCrrtfW2yEzG5ek5vPh/g0JwCD0GvZnJK1vvl4v6en+DAfAoISH8Wb2uKRbJF1uZhOSfizpQUm/NLN7JL0j6Tv9GEzNHOurV68urrtv375i/eabby7WS6Kea9Rnr5lnXCr3m6Nt1/bJa+rRfovq0X4picYd3QegpscvlecauO6664rrlvrwpXNVwrC7+90tpW9E6wIYHZwuCyRB2IEkCDuQBGEHkiDsQBJfqktcS8bHx4v1bdu2FetRm6fUoopahqXbBkv1l5GW2kCffPJJz+tK8b9JtH6pHv1eNb+3VP53iS5hjdp6g9wv0dh6vUU2z+xAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMRI9dmj3uQg1UxNHF0uGfXRo57tIG8lHW07Oocg6oXXbLt2yubS+jU9+uixpfh3K03jHU3xXdrnpd+LZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSGKk+uw1jh07VqxHffRoyubSNcbRNeNz584t1qPbFkc925r7AESiPnp0DkFpv3V5XkUk2qfR9e5Rn760/ocfflhctzRFOH12AIQdyIKwA0kQdiAJwg4kQdiBJAg7kMTQ++yl/mXN9cvbt28v1u+6665i/cSJE8V6qX8Z9WSjXnTttMo1aqdkrrkePupVR793zb3ba6+lj86tiK5JL20/2i+lbVf12c3sETM7bGa7pyx7wMwOmNmu5uO26HEAdGsmh/E/l7R2muX/4u43NB//3t9hAei3MOzuvl3S0SGMBcAA1bxBd5+Zvdoc5i9u+yEz22BmO8xsR8W2AFTqNewbJV0l6QZJByX9pO0H3X2Tu69y91U9bgtAH/QUdnc/5O7n3P28pJ9JWt3fYQHot57CbmZT50f+tqTdbT8LYDSEfXYze1zSLZIuN7MJST+WdIuZ3SDJJe2X9L2ZbrDUB4yuby71q48fP15cd8+ePcX61q1bi/W33367tRbd1z26PjkS9XRrzk+ovTd7NJd4zfXstWMbpNOnTxfrpWvOpfI9DKJ1e70ffhh2d797msWbo/UAjBZOlwWSIOxAEoQdSIKwA0kQdiCJkbrENWrF1FzquXPnzmL92WefLdZXr24/byhqvUWXO0aX10Zqbslc2/6KLt8tXeIaPXZ0O+ZBim49Pn/+/Kr1S+3U8fHx1prUew54ZgeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJIbeZx/VaXqjKZsXLlzYWnv55ZeL61555ZXF+rJly4r1I0eOFOulWw9HPf4zZ84U6zV99Kg+yKmmpbrLqaPLiiPROQSlf7PoVtILFixorZX2N8/sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5DE0PvsJVFPt+Z69qVLlxbrUW+zVL/xxhuL677wwgvF+rvvvlusX3/99cV6qV998uTJ4rrRPq+d2ri032qme5biv4dSL732NtXR2KK/p5L9+/f3XC+dN8EzO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kMVJ99po++hVXXFGsHzx4sFh/7LHHivXS9e5RD//OO+8s1l955ZVifd++fcX6ihUrWmvR2D766KNiPeoXR/9mpXrU64768NE16TX3Tqjddo1ov5SutS+NK3xmN7PlZvYbM3vDzF43sx80y5eY2fNmtrf5vDh6LADdmclh/FlJf+vuX5d0k6Tvm9nXJd0vaZu7Xy1pW/M9gBEVht3dD7r7zubr45LelLRM0h2StjQ/tkXSugGNEUAffKHX7Ga2QtJKSb+VNObuF14IvydprGWdDZI2VIwRQB/M+N14M1sg6QlJP3T3T72r45PvCkz7zoC7b3L3Ve6+qmqkAKrMKOxmNluTQd/q7k82iw+Z2XhTH5d0eDBDBNAP4WG8TfYgNkt6091/OqX0jKT1kh5sPj9dO5hoqtq1a9e21ubMmVNc96qrrirWH3rooWJ9bGzaVymS4stIJyYmivXo947aZ0ePHm2tRdNJRy3L6FLOqE1U03qrvQy1Rk0beCbrHzhwoLW2cuXK4rql24OXWoYzec3+55L+StJrZrarWfYjTYb8l2Z2j6R3JH1nBo8FoCNh2N39vyW1/Xfxjf4OB8CgcLoskARhB5Ig7EAShB1IgrADSdgwp1CeN2+eX3PNNa31m266qbj+c88911qLbr8buf3224v15cuXt9YWLVpUXPfYsWPFejSl87XXXlusv//++621Uj9Xkh5++OFi/dSpU8V6NCV06e8ruox00FM612w7qkd99tmzZ7fWon0a/a27+7SD45kdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5IYap/dzIa3MSAp+uxAcoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRBh2M1tuZr8xszfM7HUz+0Gz/AEzO2Bmu5qP2wY/XAC9Cm9eYWbjksbdfaeZLZT0kqR1mpyP/YS7//OMN8bNK4CBa7t5xUzmZz8o6WDz9XEze1PSsv4OD8CgfaHX7Ga2QtJKSb9tFt1nZq+a2SNmtrhlnQ1mtsPMdtQNFUCNGd+DzswWSPpPSf/o7k+a2ZikI5Jc0j9o8lD/b4LH4DAeGLC2w/gZhd3MZkt6VtKv3P2n09RXSHrW3f80eBzCDgxYzzectMnpKjdLenNq0Js37i74tqTdtYMEMDgzeTd+jaT/kvSapPPN4h9JulvSDZo8jN8v6XvNm3mlx+KZHRiwqsP4fiHswOBx33ggOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAS4Q0n++yIpHemfH95s2wUjerYRnVcEmPrVT/H9sdthaFez/65jZvtcPdVnQ2gYFTHNqrjkhhbr4Y1Ng7jgSQIO5BE12Hf1PH2S0Z1bKM6Lomx9WooY+v0NTuA4en6mR3AkBB2IIlOwm5ma81sj5ntM7P7uxhDGzPbb2avNdNQdzo/XTOH3mEz2z1l2RIze97M9jafp51jr6OxjcQ03oVpxjvdd11Pfz701+xmNkvS7yR9U9KEpBcl3e3ubwx1IC3MbL+kVe7e+QkYZvYXkk5I+tcLU2uZ2T9JOuruDzb/US52978bkbE9oC84jfeAxtY2zfhfq8N918/pz3vRxTP7akn73P0tdz8j6ReS7uhgHCPP3bdLOvqZxXdI2tJ8vUWTfyxD1zK2keDuB919Z/P1cUkXphnvdN8VxjUUXYR9maTfT/l+QqM137tL+rWZvWRmG7oezDTGpkyz9Z6ksS4HM41wGu9h+sw04yOz73qZ/rwWb9B93hp3/zNJfynp+83h6kjyyddgo9Q73SjpKk3OAXhQ0k+6HEwzzfgTkn7o7h9NrXW576YZ11D2WxdhPyBp+ZTvv9YsGwnufqD5fFjSU5p82TFKDl2YQbf5fLjj8fyBux9y93Pufl7Sz9ThvmumGX9C0lZ3f7JZ3Pm+m25cw9pvXYT9RUlXm9mVZjZH0nclPdPBOD7HzOY3b5zIzOZL+pZGbyrqZyStb75eL+npDsfyKaMyjXfbNOPqeN91Pv25uw/9Q9JtmnxH/n8l/X0XY2gZ159IeqX5eL3rsUl6XJOHdZ9o8r2NeyQtlbRN0l5J/yFpyQiN7VFNTu39qiaDNd7R2NZo8hD9VUm7mo/but53hXENZb9xuiyQBG/QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAS/wcBY8Nfi6jPjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted: Bag with confidence % of 0.9192290902137756\n",
      "Correct label is: Bag\n"
     ]
    }
   ],
   "source": [
    "image, label = testing_iterator.next()  # get one datapoint\n",
    "image = image.squeeze(0)  # 'squeeze' out the tensor from the outer list.\n",
    "print(\"image shape is\", image.shape)\n",
    "\n",
    "probs, gt_prob = get_probs(model, image, label, device)  # run model prediction\n",
    "\n",
    "\n",
    "# plot our image\n",
    "plt.imshow(image.view(28,28), cmap=\"gray\") \n",
    "plt.show()\n",
    "\n",
    "# what did the model predict?\n",
    "max_label = torch.argmax(probs)\n",
    "print(\"Model predicted:\", labels_map[max_label.item()], \"with confidence % of\", probs[max_label.item()].item())\n",
    "print(\"Correct label is:\", labels_map[label.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d6180-5fa1-46ee-83ae-72a15609103c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
